[{"_path":"/modules/software-development-handson/exercises_cicd","_dir":"software-development-handson","_draft":false,"_partial":false,"_locale":"","title":"CI/CD optimization","description":"","type":"exercise","order":4,"_type":"markdown","_id":"local_fs:modules:software-development-handson:exercises_cicd.md","_source":"local_fs","_file":"modules/software-development-handson/exercises_cicd.md","_stem":"modules/software-development-handson/exercises_cicd","_extension":"md","plainText":"---\ntitle: CI/CD optimization\ntype: exercise\norder: 4\n---\n\n# Make your automated testing framework use less energy\n\n## Testing is necessary\nA robust testing framework is an essential part of good software development practices. It is even more vital in a research environment since wrong or misleading scientific results may be caused by bad or insufficient testing. Automated testing aids in scientific reproducibility and reduces the amount of time wasted trying to go back to find which version still worked with a given feature.\n\nIf you are unsure about what a testing framework is or how to set one up, you can learn about it in the [Good Practices](exercises_good-practices) exercises of this hands-on module (specifically, it is [step 4](exercises_good-practices#_4-testing). Adding tests to your scientific code is a good step towards avoiding wasted energy in the long run.\n\n\n## Testing uses energy\nDespite the need for it, testing necessarily uses energy. By adding a testing suite to your software you are making the value judgement that the benefits are worth the additional energy cost of running automatic tests.\n\nBut there are ways to mimimize the amount of energy used.\nThe defaults used by most people when adding automated tests to their code base are often wasteful, due to many tests being run when it is not necessary.\n\n## Reduce energy waste from unnecessary testing\nIn the following we address some relatively simple changes that can be made to reduce the unnecessary running of tests.\nThe solutions are given for projects that use `pytest` (the ubiquitous python testing framework) and automation using features of [GitHub](www.github.com), which is a very common place to host code repositories. However, if you are using a different language (e.g. `R`) or you do not use GitHub (maybe GitLab, bitbucket etc) then the general principle should still apply. It may be possible to recreate the same changes by modifying the relevant configuration (we will try to help where we can).\n\n### 1. Cancel running workflows on a new push\nBy default, it is often the case that currently running tests continue to completion, even if you have pushed new changes to the branch. While that may be desirable in some cases, it is most often not wanted. The user is most interested in whether the newest version of a branch is passing the tests or not.\n\nLuckily, it is possible to [automatically cancel running tests if new code is pushed to the branch](https://docs.github.com/en/enterprise-cloud@latest/actions/writing-workflows/choosing-what-your-workflow-does/control-the-concurrency-of-workflows-and-jobs#example-using-concurrency-and-the-default-behavior).\n\n### 2. Only re-run tests that failed\nUse the `pytest --last-failed` command by exploiting the gh actions cache functionality using a ready-made github action:\n* pytest-last-failed: <https://github.com/sjvrijn/pytest-last-failed>\n\n### 3. Set the workflow trigger appropriately for each part of the test framework\nTest workflows are triggered to run when a particular event happens. To avoid tests running unnecessarily, it is important to ensure that this trigger is set correctly.\n\nCertain test workflows do not need to run on every push, for example, and could be configured to only run on merges to the main branch (or on tagging releases).\n\n[GitHub docs on triggering workflows](https://docs.github.com/en/actions/writing-workflows/choosing-when-your-workflow-runs/events-that-trigger-workflows)\n\n\n### 4. Test on different platforms/versions that make sense\nIt is possible to run your automated tests on several platforms (e.g. different versions of Ubuntu, MacOS and Windows) and also on different kinds of software stack (different libraries, `python` versions etc.). On GitHub, for example, this can be done using [`matrix` strategies](https://docs.github.com/en/actions/writing-workflows/choosing-what-your-workflow-does/running-variations-of-jobs-in-a-workflow). This is generally desirable, in order to ensure portability across systems.\n\nHowever, testing all these combinations comes at a cost. It is important to think about how many users you have (maybe it is just you and some colleagues) and whether what you are testing is actually necessary. For example, supporting every python version from the last 10 years is probably overkill and will waste a lot of energy.\n\n\n### 5. Create dependencies between tests\n\nContinuing to run all tests when one has failed does not always make sense. For example, if a basic linting step has failed, it may be desirable not to run the remaining test suite until that is fixed.\n\nWith GitHub actions this is possible by making dependencies between your jobs: [see relevant example here](https://docs.github.com/en/actions/writing-workflows/choosing-what-your-workflow-does/using-jobs-in-a-workflow#example-requiring-successful-dependent-jobs).\n\nAlterntively, you may wish to only run the linting tests once the basic tests pass (i.e. no point linting broken code). Which way of adding dependencies is best at reducing unnecessary tests running is dependent on your specific setup.\n\n\n### Other tools/plugins worth a look\n\nA possible alternative to `pytest-last-failed`:\n* pytest-testmon: <https://github.com/tarpas/pytest-testmon>, a pytest plugin that only runs tests that concern code that has been affected by changes (interesting but challenging, and possibly unreliable)\n\n\n## Conclusion\nTesting is essential to reducing waste in research software development, but can cause significant energy consumption itself. This is due to the way most testing frameworks are by default configured to run all of the tests, all of the time. By following some or all of the above changes, you can likely save energy by reducing a lot of pointless tests being run.\n\n"},{"_path":"/modules/software-development-handson/exercises_good-practices","_dir":"software-development-handson","_draft":false,"_partial":false,"_locale":"","title":"Good Practices","description":"Good practices in software engineering are essential to reduce energy usage. Without proper (automated) testing, documentation and sharing, resources are wasted constantly.","type":"exercise","order":3,"_type":"markdown","_id":"local_fs:modules:software-development-handson:exercises_good-practices.md","_source":"local_fs","_file":"modules/software-development-handson/exercises_good-practices.md","_stem":"modules/software-development-handson/exercises_good-practices","_extension":"md","plainText":"---\ntitle: Good Practices\ntype: exercise\norder: 3\n---\n\n# Good practices will save the earth!\n\nGood practices in software engineering are essential to reduce energy usage. Without proper (automated) testing, documentation and sharing, resources are wasted constantly. \n\nExamples are:\n- running (and failing) untested or badly tested workflows in high-resource jobs\n- having to redo a lot of work because things are lost as they were not stored/versioned properly\n- having to redo someone elses work because it was not properly published or licensed\n\n[CodeRefinery](https://coderefinery.org/) is a very good resource for learning best practices in programming and data management for research software. Their [full blown course](https://coderefinery.org/lessons/) lasts 6 days and goes fairly deep into the most important aspects of software development. \n\nThe [“CodeRefinery in 1 hour” course](https://coderefinery.github.io/research-software-engineering/) condenses most of the materials in a 1 hours crash-course. For this exercise we will follow the contents of this crash course. Below you will find the links to the six course chapters on various of research software development. \n\nBesides the links to the coderefinery course, in each of the sections below you will also find links to corresponding modules in the [Research Software Support platform.](https://esciencecenter-digital-skills.github.io/research-software-support/). These materials are less hands-on because they are made for research supporters, however they often give you a nice conceptual overview of the concepts. You can regard these as optional additional materials. \n\n## 0. Example project\n\nThe course starts with introducing an example project that will be used throughout the course. \n\nYou can also choose to follow the exercises using your own software project, in that case start with [step 1](#_1-version-control-and-code-review). Throughout the exercises, the example project will be used, we trust you can translate the steps to your own project. If you notice that this doesn't work for you, try using the example project.\n\nIf you want to use the example project, follow the link below and read up on simulating the motion of planets. \n[Example project: simulating the motion of planets](https://coderefinery.github.io/research-software-engineering/example/)\n\n## 1. Version control and code review\n\nFollow the materials and exercises from coderefinery about \n[version control and code review](https://coderefinery.github.io/research-software-engineering/version-control/).\n\nAdditional Research Software Support materials on [software version control](https://esciencecenter-digital-skills.github.io/research-software-support/modules/version-control/info)\n\n## 2. Documentation\n\nFollow the materials and exercises from coderefinery about [documentation](https://coderefinery.github.io/research-software-engineering/documentation/).\n\nAdditional Research Software Support materials on [documentation](https://esciencecenter-digital-skills.github.io/research-software-support/modules/documentation/info)\n\n\n## 3. Reproducible dependencies and environments\n\nFollow the materials and exercises from coderefinery about [reproducible dependencies and environments](https://coderefinery.github.io/research-software-engineering/reproducibility/)\n\nAdditional Research Software Support materials on [distributing software](https://esciencecenter-digital-skills.github.io/research-software-support/modules/distributing/info)\n\n\n## 4. Testing\nFollow the materials and exercises from coderefinery about [Testing](https://coderefinery.github.io/research-software-engineering/testing/)\n\nAdditional Research Software Support materials on [testing](https://esciencecenter-digital-skills.github.io/research-software-support/modules/testing/info)\n\n\n## 5. Automation and reproducible workflows\n\nFollow the materials and exercises from coderefinery about [Automation and reproducible workflows](https://coderefinery.github.io/research-software-engineering/automation/)\n\n## 6. Sharing (licensing and publishing)\n\nFollow the materials and exercises from coderefinery about [Sharing (licensing and publishing)](https://coderefinery.github.io/research-software-engineering/sharing/)\n\nAdditional Research Software Support materials on [licenses](https://esciencecenter-digital-skills.github.io/research-software-support/modules/licenses/info), [publication](https://esciencecenter-digital-skills.github.io/research-software-support/modules/publication/info) and [citation](https://esciencecenter-digital-skills.github.io/research-software-support/modules/citation/info), \n"}]