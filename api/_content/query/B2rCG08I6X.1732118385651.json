[{"_path":"/modules/hpc-handson/exercises","_dir":"hpc-handson","_draft":false,"_partial":false,"_locale":"","title":"Exercises","description":"Choose an application from the list below","type":"exercise","order":6,"_type":"markdown","_id":"local_fs:modules:hpc-handson:exercises.md","_source":"local_fs","_file":"modules/hpc-handson/exercises.md","_stem":"modules/hpc-handson/exercises","_extension":"md","plainText":"---\ntitle: Exercises\ntype: exercise\norder: 6\n---\n\n# Exercises\nChoose an application from the list [below](#applications)\n### 1. Monitor an application with EAR\n### 2. Identify whether the applicaiton is CPU intensive or Memory/Communication intensive\n### 3. Play with PyTorch Automatic Mixed Precision, and maybe model \"size\". What impacts on Energy does this have?\n\n\n## Applications\n\nAll of the Applications used in this tutorial can be found in the project space `/projects/0/energy-course/`\n\n1. [Synthetic Applications](#synthetic-applications)\n2. [Scientific Applications](#scientific-applications)\n   - [HemePure](#hemepure)\n   - [Palabos](#palabos)\n   - [GROMACS](#gromacs)\n   - [PyTorch](#pytorch)\n\n\n## Synthetic Applications\n### NAS Parallel Benchmarks (NPB3.4-MZ MPI+OpenMP) - SP-MZ Benchmark\n> The NAS Parallel Benchmarks (NPB) are a small set of programs designed to help evaluate the performance of parallel supercomputers. The benchmarks are derived from computational fluid dynamics (CFD) applications https://www.nas.nasa.gov/software/npb.html\n\n\nIn this course we will use the \"Multi-zone versions of NPB\" (NPB-MZ). These are designed to exploit multiple levels of parallelism in applications and to test the effectiveness of multi-level and hybrid parallelization (MPI-OpenMP) paradigms and tools. Specifically we use the SP-MZ (even-size zones within a problem class, increased number of zones as problem class grows).\n\n##### Problem Sizes:\n\n| Class     | Mesh size (x)  | Mesh size (y)  | Mesh size (z)  |\n| ----------- | ----------- | ----------- | ----------- |\n| C | 240 | 320 | 28 |\n| D | 1632  | 1216 | 34 |\n\nExample jobscript\n[NPB_job.sh](scripts/NPB_job.sh)\n\n\n\n## Scientific Applications\n### HemePure\n> HemePure/HemeLB developed by the team of Prof Peter Coveney at University College London (UCL), is a software pipeline that simulates blood flow. HemePure is specifically designed to efficiently handle sparse topologies, supports real-time visualization and remote steering of the simulation and can handle fully resolved realistic vessels like those found in the human brain. https://github.com/UCL-CCS/HemePure        \nhttps://github.com/UCL-CCS/HemePure-GPU\n\n* The executables are located in the directory `/projects/0/energy-course/HemePure`. There you will find the `hemepure` and `hemepure_gpu` (CUDA enabled) exectubles.\n**How to run a case**\nWe will be running through an example of pressure driven flow through a bifurcation available in the HemeLB download.\n\nCPU example jobscript\n[hemepure_cpu_job.sh](scripts/HemePure_CPU_job.sh)\n\nGPU example jobscript\n[hemepure_gpu_job.sh](scripts/HemePure_GPU_job.sh)\n\n### Palabos\n\n> The Palabos (Parallel Lattice Boltzmann Solver) library is a framework for general-purpose computational fluid dynamics (CFD), with a kernel based on the lattice Boltzmann method. The case we use in this course is a simulation of blood flow in a inside the 3D aneurysm geometry. https://palabos.unige.ch/\n\nexample jobscript\n[palabos_job.sh](scripts/Palabos_job.sh)\n\n\n### GROMACS\n> **GROMACS** A free and open-source software suite for high-performance molecular dynamics and output analysis.\n\n> **The HECBioSim Benchmarks:** (https://www.hecbiosim.ac.uk/access-hpc/benchmarks)\n\n> **HECBioSim benchmark suite** consists of a set of simple benchmarks for a number of popular Molecular Dynamics (MD) engines, each of which is set at a different atom count. The benchmark suite currently contains benchmarks for the AMBER, GROMACS, LAMMPS and NAMD molecular dynamics packages.\n\nIn this example we will choose the \"465K atom system - hEGFR Dimer of 1IVO and 1NQL\" simulation (which can be found here <https://github.com/victorusu/GROMACS_Benchmark_Suite/tree/1.0.0/HECBioSim/hEGFRDimer>). This simulation contains a total number of atoms = 465,399 (Protein atoms = 21,749  Lipid atoms = 134,268  Water atoms = 309,087  Ions = 295). The run will take about 10 minutes to execute (using all 128 cores of an AMD ROME node). The image below shows the simulation that we will run.\n\n- **20K atom system** \n```\ncurl -LJ https://github.com/victorusu/GROMACS_Benchmark_Suite/raw/1.0.0/HECBioSim/Crambin/benchmark.tpr -o Crambin_benchmark.tpr\n```\n- **1.4M atom system** \n``` \ncurl -LJ https://github.com/victorusu/GROMACS_Benchmark_Suite/raw/1.0.0/HECBioSim/hEGFRDimerPair/benchmark.tpr -o hEGFRDimerPair_benchmark.tpr\n``` \n- **3M atom system** \n```\ncurl -LJ https://github.com/victorusu/GROMACS_Benchmark_Suite/raw/1.0.0/HECBioSim/hEGFRDimerSmallerPL/benchmark.tpr -o hEGFRDimerSmallerPL_benchmark.tpr\n```\n\n\n\n\n\n### PyTorch\n> The ResNet model is based on the Deep Residual Learning for Image Recognition from this paper https://arxiv.org/abs/1512.03385 \nhttps://pytorch.org/hub/pytorch_vision_resnet/\n\n**torchvision should be installed in your environment first**\n\nExample how to install 2023\n```\nmodule load 2023\nmodule load PyTorch/2.1.2-foss-2023a-CUDA-12.1.1\nmodule load torchvision/0.16.0-foss-2023a-CUDA-12.1.1\n```\n\nExample jobscript\n[PyTorch_job.sh](scripts/PyTorch_job.sh)\n\n"},{"_path":"/modules/hpc-handson","_dir":"modules","_draft":false,"_partial":false,"_locale":"","title":"High Performance Computing","description":"","id":2,"category":"Workshop","author":"eScience Center","thumbnail":"hpc-thumbnail.avif","abstract":"Learn how to use the Energy Aware Runtime tool on Snellius to profile jobs and reduce energy usage.","order":3,"visibility":"visible","_type":"markdown","_id":"local_fs:modules:hpc-handson:index.md","_source":"local_fs","_file":"modules/hpc-handson/index.md","_stem":"modules/hpc-handson/index","_extension":"md","plainText":"---\nid: 2\ncategory: Workshop\ntitle: High Performance Computing\nauthor: eScience Center\nthumbnail: \"hpc-thumbnail.avif\"\nabstract: Learn how to use the Energy Aware Runtime tool on Snellius to profile jobs and reduce energy usage.\norder: 3\nvisibility: visible\n---\n\n# Category 1\n"}]